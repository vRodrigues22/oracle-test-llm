Oracle Test â€” LLM Semantic Test Oracle


ğŸ“˜ VisÃ£o Geral

O Oracle Test Ã© um framework para avaliaÃ§Ã£o automÃ¡tica de saÃ­das de sistemas usando:

- OrÃ¡culo Tradicional (regex / similaridade)
- OrÃ¡culo SemÃ¢ntico baseado em LLM

Suporta mÃºltiplos provedores:

- Google Gemini
- OpenAI
- LLaMA (via Ollama ou endpoint OpenAI-compatible)


ğŸ— Estrutura do Projeto:

oracle_test/

â”‚

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ .env.example

â”œâ”€â”€ README.md

â”‚

â”œâ”€â”€ config.py

â”œâ”€â”€ cache.py

â”œâ”€â”€ costs.py

â”œâ”€â”€ schemas.py

â”‚

â”œâ”€â”€ llm_oracle.py

â”œâ”€â”€ traditional_oracle.py

â”œâ”€â”€ experiment_runner.py

â”œâ”€â”€ metrics.py

â”‚

â”œâ”€â”€ prompts/

â”‚   â”œâ”€â”€ llm_system.txt

â”‚   â””â”€â”€ llm_instructions.txt

â”‚

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ test_cases.json

â”‚   â””â”€â”€ ground_truth.json

â”‚

â””â”€â”€ results/


âš™ï¸ Requisitos

Python 3.10+
Windows / Linux / MacOS
Conta em pelo menos um provedor LLM

__________________________________________________________

ğŸš€ InstalaÃ§Ã£o

1) Criar ambiente virtual:

- Windows:

python -m venv .venv

Entrar na venv:
.venv\Scripts\activate


- Linux / Mac:

python3 -m venv .venv

Entrar na venv:
source .venv/bin/activate


2) Instalar dependÃªncias:

pip install -r requirements.txt

__________________________________________________________

ğŸ” ConfiguraÃ§Ã£o .env:

1) Exemplo .env â€” Gemini:

LLM_PROVIDER=gemini
LLM_MODEL=gemini-3-flash-preview
GEMINI_API_KEY=COLE_SUA_CHAVE

LLM_ENABLE_CACHE=false
RUN_ID=RUN-001
AUTO_GENERATE_TESTS=false


2) Exemplo .env â€” OpenAI:

LLM_PROVIDER=openai
LLM_MODEL=gpt-5.2
OPENAI_API_KEY=COLE_SUA_CHAVE

âš  ChatGPT Plus NÃƒO inclui crÃ©ditos da API OpenAI.


3) Exemplo .env â€” LLaMA (Ollama):

LLM_PROVIDER=llama
LLM_MODEL=llama3
LLAMA_BASE_URL=http://localhost:11434/v1

__________________________________________________________

ğŸ“Š Rodar o experimento completo:

1) Verifique se data/ tem os arquivos

Rode:
dir data


2) Execute o experimento (com --smoke)

Rode:
python experiment_runner.py --smoke


3) Execute o experimento completo (sem --smoke)

Rode:
python experiment_runner.py

Isso deve gerar saÃ­das na pasta results/ (ex.: .jsonl e .md, dependendo do seu runner).


Depois confira:
dir results

4) Execute para validar o relatÃ³rio:

Rode:
type results\report_DATASET-001.md

